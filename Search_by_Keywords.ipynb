{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eea2d6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import tweepy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "import nltk\n",
    "# conda install -c conda-forge nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import spacy \n",
    "# conda install -c conda-forge spacy\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from gensim import corpora\n",
    "# conda install -c conda-forge gensim\n",
    "\n",
    "### Sentiment analysis\n",
    "from textblob import TextBlob\n",
    "# conda install -c conda-forge textblob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "# conda install -c conda-forge vaderSentiment\n",
    "\n",
    "info = {\"api_key\": \"EA0Nc1ZdovL6026feB6QzC42a\",\n",
    "        \"api_key_secret\": \"bbnyEELzI06L3pNCfHdLeOkTio5L6kT5KKH4eKRUXwfNrG4JQ\",\n",
    "        \"bearer_token\" : \"AAAAAAAAAAAAAAAAAAAAAA2hYgEAAAAA8OUr3Ab8wV2SBd7AQz3UxblZ%2FFM%3Dl5bbd1hC9D6tsWDN1HNtqm1SHC4Dgy56ZrjwPr4FlJWWFE8E3K\",\n",
    "        \"access_token\": \"1487509017362513921-lZvWsFfvOFrSyO1pRwMJ1nIxpuA2oi\",\n",
    "        \"access_token_secret\": \"wm7cetupMABoics4sHxrzKeC9TpDEB4mGVRMEEkqDZr72\"}\n",
    "\n",
    "client = tweepy.Client(\n",
    "    consumer_key       = info[\"api_key\"],\n",
    "    consumer_secret    = info[\"api_key_secret\"],\n",
    "    bearer_token       = info[\"bearer_token\"],\n",
    "    access_token       = info[\"access_token\"],\n",
    "    access_token_secret= info[\"access_token_secret\"],\n",
    "    wait_on_rate_limit = True\n",
    ")\n",
    "\n",
    "\n",
    "fields = \"created_at,lang,author_id,text,referenced_tweets\" \n",
    "expansions = \"attachments.media_keys,referenced_tweets.id,author_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73d8c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(search_response):\n",
    "    results  = search_response.data\n",
    "    inc_tweets = search_response.includes['tweets']\n",
    "    inc_tweets_data = [tweet.data for tweet in inc_tweets]\n",
    "    tweets = []\n",
    "    tweet_data = {}\n",
    "    for tweet in results:\n",
    "        tweet_data = tweet.data\n",
    "        tweet_data[\"is_rt\"]   = False\n",
    "        tweet_data[\"rt_id\"]   = None\n",
    "        tweet_data[\"rt_text\"] = None\n",
    "        ref_tw = tweet.get('referenced_tweets')\n",
    "        if ref_tw:\n",
    "            rt = [rt for rt in ref_tw if rt.get('type')==\"retweeted\"]\n",
    "            if len(rt) > 0:\n",
    "                rt = rt[0]\n",
    "                tweet_data[\"is_rt\"]   = True\n",
    "                tweet_data[\"rt_id\"]   = rt.data['id']  \n",
    "                tweet_data[\"rt_text\"] = [inc['text'] for inc in inc_tweets_data if inc['id']==rt.data['id']][0]\n",
    "        tweets.append(tweet_data)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f5d735",
   "metadata": {},
   "source": [
    "# keyword = homeless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2591d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "homeless_tweets = client.search_recent_tweets(\n",
    "    query=\"homeless\",\n",
    "    max_results=100,\n",
    "    tweet_fields=fields,\n",
    "    expansions=expansions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da7035b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweets = process_tweets(homeless_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5abd6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting page 0 ...\n",
      "getting page 1 ...\n",
      "getting page 2 ...\n",
      "getting page 3 ...\n",
      "getting page 4 ...\n",
      "getting page 5 ...\n",
      "getting page 6 ...\n",
      "getting page 7 ...\n",
      "getting page 8 ...\n",
      "getting page 9 ...\n",
      "getting page 10 ...\n",
      "getting page 11 ...\n",
      "getting page 12 ...\n",
      "getting page 13 ...\n",
      "getting page 14 ...\n",
      "getting page 15 ...\n",
      "getting page 16 ...\n",
      "getting page 17 ...\n",
      "getting page 18 ...\n",
      "getting page 19 ...\n",
      "getting page 20 ...\n",
      "getting page 21 ...\n",
      "getting page 22 ...\n",
      "getting page 23 ...\n",
      "getting page 24 ...\n",
      "getting page 25 ...\n",
      "getting page 26 ...\n",
      "getting page 27 ...\n",
      "getting page 28 ...\n",
      "getting page 29 ...\n",
      "getting page 30 ...\n",
      "getting page 31 ...\n",
      "getting page 32 ...\n",
      "getting page 33 ...\n",
      "getting page 34 ...\n",
      "getting page 35 ...\n",
      "getting page 36 ...\n",
      "getting page 37 ...\n",
      "getting page 38 ...\n",
      "getting page 39 ...\n",
      "getting page 40 ...\n",
      "getting page 41 ...\n",
      "getting page 42 ...\n",
      "getting page 43 ...\n",
      "getting page 44 ...\n",
      "getting page 45 ...\n",
      "getting page 46 ...\n",
      "getting page 47 ...\n",
      "getting page 48 ...\n",
      "getting page 49 ...\n",
      "getting page 50 ...\n",
      "getting page 51 ...\n",
      "getting page 52 ...\n",
      "getting page 53 ...\n",
      "getting page 54 ...\n",
      "getting page 55 ...\n",
      "getting page 56 ...\n",
      "getting page 57 ...\n",
      "getting page 58 ...\n",
      "getting page 59 ...\n",
      "getting page 60 ...\n",
      "getting page 61 ...\n",
      "getting page 62 ...\n",
      "getting page 63 ...\n",
      "getting page 64 ...\n",
      "getting page 65 ...\n",
      "getting page 66 ...\n",
      "getting page 67 ...\n",
      "getting page 68 ...\n",
      "getting page 69 ...\n",
      "getting page 70 ...\n",
      "getting page 71 ...\n",
      "getting page 72 ...\n",
      "getting page 73 ...\n",
      "getting page 74 ...\n",
      "getting page 75 ...\n",
      "getting page 76 ...\n",
      "getting page 77 ...\n",
      "getting page 78 ...\n",
      "getting page 79 ...\n",
      "getting page 80 ...\n",
      "getting page 81 ...\n",
      "getting page 82 ...\n",
      "getting page 83 ...\n",
      "getting page 84 ...\n",
      "getting page 85 ...\n",
      "getting page 86 ...\n",
      "getting page 87 ...\n",
      "getting page 88 ...\n",
      "getting page 89 ...\n",
      "getting page 90 ...\n",
      "getting page 91 ...\n",
      "getting page 92 ...\n",
      "getting page 93 ...\n",
      "getting page 94 ...\n",
      "getting page 95 ...\n",
      "getting page 96 ...\n",
      "getting page 97 ...\n",
      "getting page 98 ...\n",
      "getting page 99 ...\n"
     ]
    }
   ],
   "source": [
    "# Save multiple pages of data from a query\n",
    "# Alternative : https://docs.tweepy.org/en/stable/streamingclient.html\n",
    "next_token = None\n",
    "all_tweet_data = []\n",
    "for i in range(100):\n",
    "    if i % 1 == 0:\n",
    "        print('getting page {} ...'.format(i))\n",
    "    if next_token:\n",
    "        homeless_tweets = client.search_recent_tweets(\n",
    "            query=\"homeless\",\n",
    "            max_results=100,\n",
    "            tweet_fields=fields,\n",
    "            expansions=expansions,\n",
    "            next_token = next_token\n",
    "        )\n",
    "        tweet_data_list = process_tweets(homeless_tweets)\n",
    "    else:\n",
    "        homeless_tweets = client.search_recent_tweets(\n",
    "            query=\"homeless\",\n",
    "            max_results=100,\n",
    "            expansions=expansions,\n",
    "            tweet_fields=fields,\n",
    "        )\n",
    "        tweet_data_list = process_tweets(homeless_tweets)\n",
    "    all_tweet_data += tweet_data_list\n",
    "    next_token = homeless_tweets[3]['next_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0217fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"homeless_tweets_0414.json\", \"w\") as outfile:\n",
    "    json.dump(all_tweet_data, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215fbb99",
   "metadata": {},
   "source": [
    "# Keyword = homelessness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d89d54c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "homelessness_tweets = client.search_recent_tweets(\n",
    "    query=\"homelessness\",\n",
    "    max_results=100,\n",
    "    tweet_fields=fields,\n",
    "    expansions=expansions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e595b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweets = process_tweets(homelessness_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa0d8423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting page 0 ...\n",
      "getting page 1 ...\n",
      "getting page 2 ...\n",
      "getting page 3 ...\n",
      "getting page 4 ...\n",
      "getting page 5 ...\n",
      "getting page 6 ...\n",
      "getting page 7 ...\n",
      "getting page 8 ...\n",
      "getting page 9 ...\n",
      "getting page 10 ...\n",
      "getting page 11 ...\n",
      "getting page 12 ...\n",
      "getting page 13 ...\n",
      "getting page 14 ...\n",
      "getting page 15 ...\n",
      "getting page 16 ...\n",
      "getting page 17 ...\n",
      "getting page 18 ...\n",
      "getting page 19 ...\n",
      "getting page 20 ...\n",
      "getting page 21 ...\n",
      "getting page 22 ...\n",
      "getting page 23 ...\n",
      "getting page 24 ...\n",
      "getting page 25 ...\n",
      "getting page 26 ...\n",
      "getting page 27 ...\n",
      "getting page 28 ...\n",
      "getting page 29 ...\n",
      "getting page 30 ...\n",
      "getting page 31 ...\n",
      "getting page 32 ...\n",
      "getting page 33 ...\n",
      "getting page 34 ...\n",
      "getting page 35 ...\n",
      "getting page 36 ...\n",
      "getting page 37 ...\n",
      "getting page 38 ...\n",
      "getting page 39 ...\n",
      "getting page 40 ...\n",
      "getting page 41 ...\n",
      "getting page 42 ...\n",
      "getting page 43 ...\n",
      "getting page 44 ...\n",
      "getting page 45 ...\n",
      "getting page 46 ...\n",
      "getting page 47 ...\n",
      "getting page 48 ...\n",
      "getting page 49 ...\n",
      "getting page 50 ...\n",
      "getting page 51 ...\n",
      "getting page 52 ...\n",
      "getting page 53 ...\n",
      "getting page 54 ...\n",
      "getting page 55 ...\n",
      "getting page 56 ...\n",
      "getting page 57 ...\n",
      "getting page 58 ...\n",
      "getting page 59 ...\n",
      "getting page 60 ...\n",
      "getting page 61 ...\n",
      "getting page 62 ...\n",
      "getting page 63 ...\n",
      "getting page 64 ...\n",
      "getting page 65 ...\n",
      "getting page 66 ...\n",
      "getting page 67 ...\n",
      "getting page 68 ...\n",
      "getting page 69 ...\n",
      "getting page 70 ...\n",
      "getting page 71 ...\n",
      "getting page 72 ...\n",
      "getting page 73 ...\n",
      "getting page 74 ...\n",
      "getting page 75 ...\n",
      "getting page 76 ...\n",
      "getting page 77 ...\n",
      "getting page 78 ...\n",
      "getting page 79 ...\n",
      "getting page 80 ...\n",
      "getting page 81 ...\n",
      "getting page 82 ...\n",
      "getting page 83 ...\n",
      "getting page 84 ...\n",
      "getting page 85 ...\n",
      "getting page 86 ...\n",
      "getting page 87 ...\n",
      "getting page 88 ...\n",
      "getting page 89 ...\n",
      "getting page 90 ...\n",
      "getting page 91 ...\n",
      "getting page 92 ...\n",
      "getting page 93 ...\n",
      "getting page 94 ...\n",
      "getting page 95 ...\n",
      "getting page 96 ...\n",
      "getting page 97 ...\n",
      "getting page 98 ...\n",
      "getting page 99 ...\n"
     ]
    }
   ],
   "source": [
    "# Save multiple pages of data from a query\n",
    "# Alternative : https://docs.tweepy.org/en/stable/streamingclient.html\n",
    "next_token = None\n",
    "all_tweet_data = []\n",
    "for i in range(100):\n",
    "    if i % 1 == 0:\n",
    "        print('getting page {} ...'.format(i))\n",
    "    if next_token:\n",
    "        homelessness_tweets = client.search_recent_tweets(\n",
    "            query=\"homelessness\",\n",
    "            max_results=100,\n",
    "            tweet_fields=fields,\n",
    "            expansions=expansions,\n",
    "            next_token = next_token\n",
    "        )\n",
    "        tweet_data_list = process_tweets(homelessness_tweets)\n",
    "    else:\n",
    "        homelessness_tweets = client.search_recent_tweets(\n",
    "            query=\"homelessness\",\n",
    "            max_results=100,\n",
    "            expansions=expansions,\n",
    "            tweet_fields=fields,\n",
    "        )\n",
    "        tweet_data_list = process_tweets(homelessness_tweets)\n",
    "    all_tweet_data += tweet_data_list\n",
    "    next_token = homelessness_tweets[3]['next_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60a8cef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"homelessness_tweets_0414.json\", \"w\") as outfile:\n",
    "    json.dump(all_tweet_data, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d6376e",
   "metadata": {},
   "source": [
    "# Keyword = \"covid\" + \"homeless\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9ff3712",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmlscvd_tweets = client.search_recent_tweets(\n",
    "    query=\"homeless covid\",\n",
    "    max_results=100,\n",
    "    tweet_fields=fields,\n",
    "    expansions=expansions\n",
    ")\n",
    "\n",
    "processed_tweets = process_tweets(hmlscvd_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c0781e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting page 0 ...\n",
      "getting page 1 ...\n",
      "getting page 2 ...\n",
      "getting page 3 ...\n",
      "getting page 4 ...\n",
      "getting page 5 ...\n",
      "getting page 6 ...\n",
      "getting page 7 ...\n",
      "getting page 8 ...\n",
      "getting page 9 ...\n",
      "getting page 10 ...\n",
      "getting page 11 ...\n",
      "getting page 12 ...\n",
      "getting page 13 ...\n",
      "getting page 14 ...\n",
      "getting page 15 ...\n",
      "getting page 16 ...\n",
      "getting page 17 ...\n",
      "getting page 18 ...\n",
      "getting page 19 ...\n",
      "getting page 20 ...\n",
      "getting page 21 ...\n",
      "getting page 22 ...\n",
      "getting page 23 ...\n",
      "getting page 24 ...\n",
      "getting page 25 ...\n",
      "getting page 26 ...\n",
      "getting page 27 ...\n",
      "getting page 28 ...\n",
      "getting page 29 ...\n",
      "getting page 30 ...\n",
      "getting page 31 ...\n",
      "getting page 32 ...\n",
      "getting page 33 ...\n"
     ]
    }
   ],
   "source": [
    "next_token = None\n",
    "all_tweet_data = []\n",
    "for i in range(34):\n",
    "    if i % 1 == 0:\n",
    "        print('getting page {} ...'.format(i))\n",
    "    if next_token:\n",
    "        hmlscvd_tweets = client.search_recent_tweets(\n",
    "            query=\"homeless covid\",\n",
    "            max_results=100,\n",
    "            tweet_fields=fields,\n",
    "            expansions=expansions,\n",
    "            next_token = next_token\n",
    "        )\n",
    "        tweet_data_list = process_tweets(hmlscvd_tweets)\n",
    "    else:\n",
    "        homelessness_tweets = client.search_recent_tweets(\n",
    "            query=\"homeless covid\",\n",
    "            max_results=100,\n",
    "            expansions=expansions,\n",
    "            tweet_fields=fields,\n",
    "        )\n",
    "        tweet_data_list = process_tweets(hmlscvd_tweets)\n",
    "    all_tweet_data += tweet_data_list\n",
    "    next_token = hmlscvd_tweets[3]['next_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bd67aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hmlscvd_tweets_0419.json\", \"w\") as outfile:\n",
    "    json.dump(all_tweet_data, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f2cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tweets from 04/11 to 04/19"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
